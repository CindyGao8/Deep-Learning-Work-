{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6."
      ],
      "metadata": {
        "id": "D9cvMTXjPdoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.\n",
        "Episodic tasks consist of distinct episodes with a clear start and end, where actions within one episode do not affect future ones. In contrast, continuous tasks proceed indefinitely or for an extended period without explicit resets, and decisions made at any point can influence the task's progression over time. These distinctions impact how problems are modeled and solved, particularly in fields like reinforcement learning.\n",
        "\n",
        "\n",
        "Playing a game of chess is a classic example of an episodic task. Each game starts with the same board setup, and players make moves until the game ends in a win, loss, or draw. Once the game concludes, the outcome does not affect the next game, as each episode is treated independently of others.\n",
        "\n",
        "\n",
        "Driving a car in a simulation exemplifies a continuous task. The simulation has no fixed endpoint and keeps running as long as the car operates. Each decision, such as accelerating, steering, or braking, affects the car's position, speed, and environment, influencing future decisions in a continuous loop.\n"
      ],
      "metadata": {
        "id": "ZRbm3ScoPw2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2."
      ],
      "metadata": {
        "id": "eQuwMS2BQY1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1."
      ],
      "metadata": {
        "id": "IvhWRlGvQ1GP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration refers to the agent’s strategy of trying new actions to discover their effects and potential rewards in the environment. The goal of exploration is to gather more information about the environment, which is crucial when the agent is uncertain about the outcomes of certain actions. By exploring, the agent can avoid getting stuck in a suboptimal solution and potentially find better policies. A classic example is a robot randomly choosing paths in a maze to map all possible routes.\n",
        "\n",
        "Exploitation, on the other hand, involves the agent using the knowledge it has already acquired to select the action that it believes will maximize the immediate reward. This approach prioritizes using what the agent already knows to achieve higher rewards rather than learning new information. For instance, if a robot knows a particular path leads to a treasure, it will keep choosing that path to maximize its reward.\n",
        "\n"
      ],
      "metadata": {
        "id": "tx_ZCY_OQa0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2."
      ],
      "metadata": {
        "id": "1LPhqOBzQ3aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Exploration: By choosing a random action with a probability of ϵ, the actor ensures that it occasionally explores less familiar or suboptimal actions.\n",
        "\n",
        "2.   Exploitation: By selecting the action with the highest estimated value (based on the current Q-values or policy) with a probability of 1−ϵ, the actor leverages its existing knowledge to maximize immediate rewards.   "
      ],
      "metadata": {
        "id": "dZmidHSKQyId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3\n",
        "\n",
        "ε should, in general, follow a schedule and not be fixed. Having a fixed ε keeps a constant balance between exploration and exploitation, but it is not efficient for learning because the agent will either explore too much or not enough if ε is high or low, respectively. Actors use the ϵ-greedy policy to trade off between exploration (selecting actions to learn their rewards) and exploitation (selecting the best-learned action to maximize immediate rewards) for more effective learning."
      ],
      "metadata": {
        "id": "rY9TFvlP0Ypt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.\n",
        "When ϵ is high, the agent favors exploration by taking random actions more frequently. On the other hand, when ϵ is low, the agent favors exploitation, choosing the actions with the highest estimated value according to current knowledge. As ϵ gradually decreases in the learning process, the agent shifts toward exploitation and focuses on receiving rewards to the maximum, basing its actions on knowledge learned during exploration."
      ],
      "metadata": {
        "id": "x0hlJ6OC0tjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part3\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Initialization of Replay Memory and Q-Network:** Initialize with a replay memory and a Q-network with random weights. In Q-Learning, there is no concept of replay memory, but instead, a Q-table is maintained; instead of a neural network, a\n",
        "\n",
        "2. **Select Action Using ϵ-Greedy Policy:** Generate actions by following the ϵ-greedy policy.\n",
        "\n",
        "3. **Perform Action and Store Experience**: Execute the selected action, and store the experience concerning state, action, reward, and next state in the replay memory. For Q-Learning, instantly after seeing an experience (and not employing a replay memory), an update is performed to the Q-table.\n",
        "\n",
        "4. **Sample Mini-batches:** From the replay memory, extract mini-batches of experiences used for training. In Q-Learning, updates will only depend on the very last experience seen.\n",
        "\n",
        "5. **Calculate Target Q-Value and Minimize Loss:** Determine the target Q-value and update the weights of the network by minimizing the loss. Q-learning updates the Q-values directly through the Bellman equation.\n",
        "\n",
        "6. **Periodic Update Target Network:** Periodically align the target network with the main Q-network to make updates stable. Q-Learning does not use a target network; hence, the update is not that stable."
      ],
      "metadata": {
        "id": "OUcq-Xt6UQ3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part4.\n",
        "The target Q-network helps in improving training stability by providing a fixed reference for updating the Q-values. The target calculation is decoupled from the online network, and gains less feedback loop, which might cause oscillation in the process. It updates the target network occasionally to ensure smoother convergence in a high-dimensional environment."
      ],
      "metadata": {
        "id": "heLemS9LUyZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5\n",
        "Experience replay enhances Q-Learning in that it stores past experiences in a buffer and randomly samples them during training. This disrupts the sequence of highly correlated data and allows it to learn from a large set of experiences. More importantly, it gains much in efficiency by reusing past data instead of constant new interactions with the environment. It will enable the agent to learn from previous experiences, albeit those generated under different conditions or policies, thereby assuring more stability and efficiency in learning."
      ],
      "metadata": {
        "id": "B6SKgbrAE1Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 6.\n",
        "Prioritized Experience Replay is an enhancement to the traditional experience replay mechanism in reinforcement learning, where experiences in the replay buffer are sampled based on their importance rather than uniformly at random.  This method ensures that the agent focuses more on learning from significant experiences, such as those with higher temporal-difference (TD) errors, which indicate greater discrepancies between the predicted Q-value and the target Q-value.  Prioritizing these experiences accelerates learning by addressing critical mistakes more frequently.\n",
        "\n",
        "Priority for different experiences in prioritized experience replay is calculated based on their importance to learning, which is typically determined by how surprising or significant the experience is.  This importance is measured by the difference between the predicted value of taking an action in a given state and the actual observed outcome.  Experiences with larger differences, or errors, are considered more critical because they indicate areas where the agent's current understanding is incorrect or incomplete."
      ],
      "metadata": {
        "id": "D_vcdN2KFPTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7.\n",
        "\n",
        "GORILA and Ape-X share key similarities as distributed reinforcement learning architectures. Both leverage distributed learning by employing multiple actors that interact with the environment in parallel, generating a diverse range of experiences sent to a centralized learner. This design accelerates training by efficiently collecting large volumes of data. Additionally, both architectures utilize a single centralized learner to update the policy or Q-network, ensuring streamlined model optimization. They are also highly scalable, capable of handling numerous distributed actors to tackle complex tasks through parallel computation.\n",
        "\n",
        "The primary differences between GORILA and Ape-X lie in their experience replay and actor-learner interactions. While GORILA relies on a shared replay buffer with uniform sampling, Ape-X introduces prioritized experience replay, focusing on experiences with higher TD errors to enhance learning efficiency. Furthermore, GORILA features loosely coupled actors and learners, leading to less frequent policy updates for actors, whereas Ape-X ensures actors sample actions directly from the latest policy, aligning their behavior with the current learning state. In terms of complexity, GORILA requires managing multiple learners and replay buffers, making its implementation more intricate compared to Ape-X, which employs a single learner and centralized prioritized replay buffer for simplicity and scalability."
      ],
      "metadata": {
        "id": "pjtOcrXBFgWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7"
      ],
      "metadata": {
        "id": "4jPVEB7EGC8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.\n",
        "1. IBM Watson (watsonx.ai):\n",
        "\n",
        "- TensorFlow: 2.14.1\n",
        "- PyTorch: 2.1.2\n",
        "- TensorBoard: 2.12.2\n",
        "- torchVision: 0.15.2\n",
        "- OpenCV: 4.7.0\n",
        "- scikit-learn: 1.1.3\n",
        "- XGBoost: 1.7.6\n",
        "- ONNX: 1.13.1\n",
        "- PyArrow: 11.0.0\n",
        "- Python: 3.10.12\n",
        "\n",
        "2. Google Cloud AI Platform (Vertex AI):\n",
        "- Base: Versions up to CUDA 12.3\n",
        "- TensorFlow: Versions up to 2.17.0\n",
        "- PyTorch: Versions up to 2.3.0\n",
        "\n",
        "3. Microsoft Azure AI:\n",
        "\n",
        "- TensorFlow: 2.5\n",
        "- PyTorch: 1.9.0\n",
        "- CUDA, cuDNN, NVIDIA Driver: CUDA 11\n",
        "- Horovod: 0.21.3\n",
        "- scikit-learn: 0.20.3\n",
        "\n",
        "4. Amazon Web Services (AWS) AI/ML:\n",
        "\n",
        "- TensorFlow: Versions up to 2.6\n",
        "- PyTorch: Versions up to 2.0\n",
        "- Apache MXNet: Version 1.8\n",
        "- XGBoost: Version 1.7.6\n",
        "- SageMaker AI Generic Estimator"
      ],
      "metadata": {
        "id": "lDD0j_2yGF2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part2.\n",
        "1. IBM Watson:\n",
        "\n",
        "Compute Units Offered: IBM's watsonx platform is designed to be flexible and can be deployed across multiple cloud environments, including Microsoft Azure and AWS. This flexibility allows users to leverage the GPU offerings of these cloud providers when deploying watsonx services.\n",
        "- NVIDIA H100\n",
        "- NVIDIA L40S\n",
        "- NVIDIA L4\n",
        "- NVIDIA P100\n",
        "- NVIDIA T4\n",
        "\n",
        "\n",
        "2. Google Cloud AI Platform:\n",
        "\n",
        "Compute Units Offered: Google Cloud offers a range of GPUs, including NVIDIA's A100, V100, P100, and T4 Tensor Core GPUs, to cater to various machine learning and deep learning workloads. These GPUs are available across different machine types and can be customized based on performance requirements.\n",
        "- NVIDIA A100\n",
        "- NVIDIA V100\n",
        "- NVIDIA P100\n",
        "- NVIDIA T4\n",
        "- NVIDIA P4\n",
        "- NVIDIA K80\n",
        "- Tensor Processing Units (TPUs)\n",
        "\n",
        "\n",
        "3. Microsoft Azure AI:\n",
        "\n",
        "Compute Units Offered: Azure provides a variety of GPU options, such as NVIDIA's A100, V100, P100, and K80 GPUs, through its NC, ND, and NV series virtual machines. These VMs are tailored for different AI and machine learning tasks, offering flexibility in performance and cost.\n",
        "- NVIDIA A100\n",
        "- NVIDIA V100\n",
        "- NVIDIA T4\n",
        "- NVIDIA P100\n",
        "- NVIDIA K80\n",
        "- Field-Programmable Gate Arrays (FPGAs)\n",
        "\n",
        "\n",
        "4. Amazon Web Services (AWS) AI/ML:\n",
        "\n",
        "Compute Units Offered: AWS offers a comprehensive selection of GPU instances, including NVIDIA's A100, V100, K80, and T4 Tensor Core GPUs, through its P4, P3, P2, and G4 instance families. Additionally, AWS has developed its own Trainium chips, designed specifically for high-performance AI training workloads, providing an alternative to traditional GPUs.\n",
        "- NVIDIA A100\n",
        "- NVIDIA V100 (P3 instances)\n",
        "- NVIDIA T4 (G4 instances)\n",
        "- NVIDIA K80\n",
        "- AWS Trainium chips\n",
        "- NVIDIA H100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8CpMVpv7GFzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part3.\n",
        "\n",
        "**IBM Watson (watsonx.ai):**\n",
        "- IBM Watson Studio\n",
        "- Watson Machine Learning\n",
        "- ModelOps (for deployment and monitoring)\n",
        "- AI Factsheets (for model documentation and governance)\n",
        "\n",
        "**Google Cloud AI Platform (Vertex AI):**\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI Pipelines\n",
        "- Vertex AI Monitoring\n",
        "- Vertex Explainable AI\n",
        "\n",
        "**Microsoft Azure AI:**\n",
        "- Azure Machine Learning Studio\n",
        "- Azure ML Pipelines\n",
        "- Azure ML Model Registry\n",
        "- Azure Monitor (for deployment and monitoring)\n",
        "\n",
        "**Amazon Web Services (AWS) AI/ML:**\n",
        "- Amazon SageMaker Model Registry\n",
        "- Amazon SageMaker Pipelines\n",
        "- Amazon SageMaker Clarify (for bias detection)\n",
        "- Amazon SageMaker Model Monitor\n"
      ],
      "metadata": {
        "id": "4kkPymypGFwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4.\n",
        "\n",
        "\n",
        "**IBM Watson (watsonx.ai):**\n",
        "- Application logs accessible through Watson Studio and Watson Machine Learning.\n",
        "- Resource usage monitoring for GPU, CPU, and memory via integrated dashboards.\n",
        "- Alerts and notifications for anomalies in resource consumption or application performance.\n",
        "\n",
        "**Google Cloud AI Platform (Vertex AI):**\n",
        "- Application logs available via Google Cloud Logging.\n",
        "- Resource usage (GPU, CPU, memory) monitoring through Google Cloud Monitoring.\n",
        "- Customizable dashboards and alerts to track model and infrastructure performance.\n",
        "\n",
        "**Microsoft Azure AI:**\n",
        "- Application logs managed through Azure Monitor.\n",
        "- Resource usage monitoring for GPU, CPU, and memory integrated with Azure Insights.\n",
        "- Automated alerts and visualizations for performance metrics via Azure Portal.\n",
        "\n",
        "**Amazon Web Services (AWS) AI/ML:**\n",
        "- Application logs accessible via Amazon CloudWatch Logs.\n",
        "- GPU, CPU, and memory usage monitoring provided through Amazon CloudWatch.\n",
        "- Custom alerts and dashboards for detailed performance and resource tracking."
      ],
      "metadata": {
        "id": "yJqMaBH5GFoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part5.\n",
        "### Visualization of Training Metrics by Major ML Cloud Platforms\n",
        "\n",
        "**IBM Watson (watsonx.ai):**\n",
        "- Performance metrics like accuracy, loss, and throughput visualized in real-time via Watson Studio.\n",
        "- Customizable dashboards to monitor training progress and analyze key metrics.\n",
        "\n",
        "**Google Cloud AI Platform (Vertex AI):**\n",
        "- Metrics such as accuracy, precision, recall, and loss visualized during training through TensorBoard integration.\n",
        "- Vertex AI Pipelines provide logs and visual summaries of model performance throughout the training process.\n",
        "\n",
        "**Microsoft Azure AI:**\n",
        "- Training metrics (e.g., accuracy and loss) visualized in real-time through Azure ML Studio dashboards.\n",
        "- Support for TensorBoard to track detailed model training metrics and throughput.\n",
        "\n",
        "**Amazon Web Services (AWS) AI/ML:**\n",
        "- Metrics like accuracy, loss, and throughput visualized via Amazon SageMaker Debugger.\n",
        "- Integration with TensorBoard for detailed real-time tracking of training progress and performance metrics."
      ],
      "metadata": {
        "id": "c40zevZdKMN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6.\n",
        "\n",
        "\n",
        "\n",
        "*  Example Training Job: Image Classification\n",
        "Task: Train a ResNet50 model on the CIFAR-10 dataset.\n",
        "Configuration:\n",
        "Dataset: cifar10\n",
        "Framework: TensorFlow\n",
        "Batch size: 32\n",
        "Epochs: 10\n",
        "Learning rate: 0.001\n",
        "Compute: 1 NVIDIA Tesla V100 GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "76GUX82KKdXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. IBM Watson (watsonx.ai) File Format: YAML"
      ],
      "metadata": {
        "id": "s0ThWhNzKwgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_job:\n",
        "  name: \"image-classification-job\"\n",
        "  framework: \"tensorflow\"\n",
        "  framework_version: \"2.14.1\"\n",
        "  runtime: \"python3.10\"\n",
        "  compute:\n",
        "    gpu: 2\n",
        "    memory: \"16GB\"\n",
        "  data:\n",
        "    source: \"cos://bucket-name/dataset/\"\n",
        "    target: \"/mnt/dataset\"\n",
        "  hyperparameters:\n",
        "    batch_size: 32\n",
        "    epochs: 10\n",
        "    learning_rate: 0.001\n",
        "  output:\n",
        "    model_path: \"cos://bucket-name/model-output/\"\n"
      ],
      "metadata": {
        "id": "x49p5BZ4KLfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Google Cloud AI Platform (Vertex AI)\n",
        "File Format: JSON\n",
        "\n"
      ],
      "metadata": {
        "id": "gyRBR1HsK2DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"jobId\": \"image-classification-job\",\n",
        "  \"trainingInput\": {\n",
        "    \"scaleTier\": \"CUSTOM\",\n",
        "    \"masterConfig\": {\n",
        "      \"imageUri\": \"tensorflow:2.14.1\",\n",
        "      \"acceleratorConfig\": {\n",
        "        \"count\": 2,\n",
        "        \"type\": \"NVIDIA_TESLA_A100\"\n",
        "      }\n",
        "    },\n",
        "    \"workerConfig\": {\n",
        "      \"replicaCount\": 2,\n",
        "      \"machineType\": \"n1-standard-4\"\n",
        "    },\n",
        "    \"args\": [\"--batch_size=32\", \"--epochs=10\", \"--learning_rate=0.001\"],\n",
        "    \"region\": \"us-central1\"\n",
        "  },\n",
        "  \"inputDataConfig\": {\n",
        "    \"uri\": \"gs://bucket-name/dataset/\"\n",
        "  },\n",
        "  \"outputDataConfig\": {\n",
        "    \"uri\": \"gs://bucket-name/model-output/\"\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "U5ExxQldK6UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Microsoft Azure AI\n",
        "File Format: YAML"
      ],
      "metadata": {
        "id": "VUXJgAFpK-Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job:\n",
        "  name: \"image-classification-job\"\n",
        "  environment: \"AzureML-TensorFlow-2.14.1\"\n",
        "  compute: \"gpu-cluster\"\n",
        "  resources:\n",
        "    gpu: 2\n",
        "    memory: \"16GB\"\n",
        "  input_data:\n",
        "    - id: \"dataset\"\n",
        "      path: \"azureml://datastore/dataset/\"\n",
        "  hyperparameters:\n",
        "    batch_size: 32\n",
        "    epochs: 10\n",
        "    learning_rate: 0.001\n",
        "  output_data:\n",
        "    model: \"azureml://datastore/model-output/\"\n"
      ],
      "metadata": {
        "id": "KcliEQbLLBn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. AWS SageMaker\n",
        "File Format:  JSON\n"
      ],
      "metadata": {
        "id": "6l8O1q9hLESz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"TrainingJobName\": \"image-classification-job\",\n",
        "  \"AlgorithmSpecification\": {\n",
        "    \"TrainingImage\": \"tensorflow:2.14.1\",\n",
        "    \"TrainingInputMode\": \"File\"\n",
        "  },\n",
        "  \"ResourceConfig\": {\n",
        "    \"InstanceType\": \"ml.p3.2xlarge\",\n",
        "    \"InstanceCount\": 1,\n",
        "    \"VolumeSizeInGB\": 50\n",
        "  },\n",
        "  \"HyperParameters\": {\n",
        "    \"batch_size\": \"32\",\n",
        "    \"epochs\": \"10\",\n",
        "    \"learning_rate\": \"0.001\"\n",
        "  },\n",
        "  \"InputDataConfig\": [\n",
        "    {\n",
        "      \"ChannelName\": \"training\",\n",
        "      \"DataSource\": {\n",
        "        \"S3DataSource\": {\n",
        "          \"S3Uri\": \"s3://bucket-name/dataset/\",\n",
        "          \"S3DataType\": \"S3Prefix\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"OutputDataConfig\": {\n",
        "    \"S3OutputPath\": \"s3://bucket-name/model-output/\"\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "cRI85NIjLGwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}